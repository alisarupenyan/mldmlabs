{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pAJRKdv9QA4C",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alisarupenyan/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "\n",
    "cmap= mpl.colors.ListedColormap(['red', 'black', 'blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfTwSIFT9UyK"
   },
   "source": [
    "In order to plot the decision boundary and margins we will used the `DecisionBoundaryDisplay.from_estimator` from `sklearn` and wrap it into a function for reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ej03R1G9UyM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, X, y, ax):\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap)\n",
    "\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        estimator=model, X=X,ax=ax,\n",
    "        response_method=\"decision_function\",\n",
    "        plot_method=\"contour\",\n",
    "        levels=[-1, 0, 1],\n",
    "        colors=[\"grey\", \"black\", \"grey\"],\n",
    "        linestyles=[\"--\", \"-\", \"--\"],\n",
    "    )\n",
    "\n",
    "    # indicate the support vectors\n",
    "    ax.scatter(X[model.support_][:, 0], X[model.support_][:, 1],\n",
    "               facecolors='none', edgecolors='black', s=100)\n",
    "\n",
    "    ax.set_xlabel(r'$x_1$', fontsize=16)\n",
    "    ax.set_ylabel(r'$x_2$', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTVOKVltsG4d"
   },
   "source": [
    "# TASK 1. SVM (2 Points): Soft Margin Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA0LWVdnQzlU"
   },
   "source": [
    "Let's work on the same artificial data with two classes from part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbvdP62EIWQF",
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=cmap)\n",
    "ax.set_xlabel(r'$x_1$', fontsize=16)\n",
    "ax.set_ylabel(r'$x_2$', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Lkm6FC-1KsD",
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='linear', C=0.1) # initialize the model\n",
    "model.fit(X, y) # fit the model = learn the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-ZXrmEi9UyO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "plot_svc_decision_function(model, X, y, ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd8_ENRCi5ch"
   },
   "source": [
    "In the plot you see a solid and 2 dashed lines.\n",
    "\n",
    "The solid line shows the decision boundary, meaning points on one side will be assigned to the blue class and on the other side to the red class. In this case all\n",
    "data points lie on the correct side of the decision boundary, meaning our classifier has successfully learned to separate the classes.\n",
    "\n",
    "The dashed lines visualize the margin of the SVM classifier. You can see that a\n",
    "few points lie inside the margin. This is because, in practice, we use a soft-margin implementation for SVM. This means that we allow data points to be inside the margin, or even on the wrong side of the boundary, but during training we \"punish\" these points.\n",
    "\n",
    "In the next excercise we will investigate the effect of the `C` parameter of. This\n",
    "parameter controls how much we \"punish\" points inside (or on the wrong side) of the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0JJcOdK1KsE"
   },
   "source": [
    "### Regularization\n",
    "For each value `C` given in the code fragment below create an instance of SVC and set its parameter C accordingly. Train the model on the data `X` from the cells above. Plot the decision boundary using `plot_svc_decision_function(model, X, y, ax)`. Observe the effect from the plots and discuss with your colleagues whether it is in line with your expectation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sn2zmKrr1KsE"
   },
   "outputs": [],
   "source": [
    "for c in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    # create an instance of SVC and set its parameter C\n",
    "    # call its .fit method to train it on X and y\n",
    "\n",
    "    model = ...\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(5, 5))\n",
    "    plot_svc_decision_function(model, X, y, ax)\n",
    "    ax.set_title(f\"C={c}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l9Ebxgp1KsE"
   },
   "source": [
    "### We add a new datapoint\n",
    "\n",
    "We will now add a new red data point, that lies very close to the cluster of blue points.\n",
    "\n",
    "In real world datasets, we run into this situation all the time. One reason can be that some data points really do look very similar but belong to different classes (e.g. imagine distinguishing a crocodile from an alligator). Another reason could be that the data point would really belong to the other (blue) class but has been annotated badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otatVaHS1KsF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X2 = np.append(X, [[1., 1.5]], axis=0)\n",
    "y2 = np.append(y, [0], axis=0)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X2[:, 0], X2[:, 1], c=y2, s=50, cmap=cmap)\n",
    "ax.set_xlabel(r'$x_1$', fontsize=16)\n",
    "ax.set_ylabel(r'$x_2$', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Awtugu1h1KsF"
   },
   "source": [
    "Rerun the examle from above, i.e. train a linear `SVC` with different values of `C` and see what changes.\n",
    "\n",
    "Pay particular attention to how the new data point changes the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9Nod5tv1KsF"
   },
   "outputs": [],
   "source": [
    "for c in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    # create an instance of LinearSVC and set its parameter C\n",
    "    # call its .fit method to train it on X2 and y2\n",
    "\n",
    "    model = ...\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(5, 5))\n",
    "    plot_svc_decision_function(model, ...)\n",
    "    ax.set_title(f\"C={c}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON8ogrmw1q4k"
   },
   "source": [
    "\n",
    "\n",
    "Based on the experiments in **task 1** above, which value for `C` would you choose for the new data, and **why**?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBJKSAuw79su"
   },
   "source": [
    "# Task 2. SVM Kernel Trick (6 points)\n",
    "In this task we will investigate the case of non-linearly separable data. In order to handle such a case, the **Kernel Trick** can be used. We transform our data and map it into a **higher dimensional feature space** (e.g., if the data had two features (2D-space), it becomes 3D-space). The goal is that after the transformation to the higher dimensional space, the classes will be linearly separable. The decision boundary can then be fitted to separate the classes and make predictions. The decision boundary will be a hyperplane in this higher dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1be1nzl1KsJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_non_linear, y_non_linear = datasets.make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(X_non_linear[:, 0], X_non_linear[:, 1], c=y_non_linear, cmap=cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBDHhAPb1KsJ"
   },
   "source": [
    "### Default model\n",
    "\n",
    "Below you can see what happens if we naively train a linear `SVC` model on this non-linear dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XI_lTRAu1KsJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear').fit(X_non_linear, y_non_linear)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_svc_decision_function(clf, X_non_linear, y_non_linear, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAYJ2Qxf8CFi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_non_linear, y_pred=clf.predict(X_non_linear), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3RtWVcn1KsJ"
   },
   "source": [
    "### Manually Adding a Helper Dimension\n",
    "\n",
    "It is clear that no linear discrimination will ever be able to separate this data. We can think about how we might project the data into a higher dimension such that a linear separator would be sufficient. In the code below we compute a new value `r` based on the data points. Adding `r` as a new dimension to our data, we will see that the data becomes linearly separable.\n",
    "\n",
    "We compute $r = e^{-||x||^2}$. We chose this because the data points lie on circles and $||x||^{2}$ corresponds to the radius of the circle that a data point lies on. The exponential is often used for numerical stability. However in this case, the picture would look the same when plotting $-||x||^{2}$ directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJ_cZCwx1KsJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = np.exp(-(X_non_linear ** 2).sum(1))\n",
    "\n",
    "fig1=plt.figure(figsize=(10, 10))\n",
    "ax = fig1.add_subplot(projection='3d')\n",
    "ax.scatter3D(X_non_linear[:, 0], X_non_linear[:, 1], r,\n",
    "             c=y_non_linear, s=50, cmap=cmap)\n",
    "ax.view_init(elev=15, azim=60)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XR6byXE9UyU"
   },
   "source": [
    "### 2A Linear Classification in the extended feature space\n",
    "\n",
    "Task: Append the additional feature `r` to the original data `X_non_linear` and train a linear `SVC` in the extended feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XK1zorHy9UyU"
   },
   "outputs": [],
   "source": [
    "X_3d = ...\n",
    "clf_3d = svm.SVC(kernel='linear').fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9la9uIBQ9UyV",
    "tags": []
   },
   "source": [
    "Have the accuracy metrics improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-srCiuB9UyV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_non_linear, y_pred=clf_3d.predict(X_3d), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvkUZ3ZS9UyV"
   },
   "source": [
    "We can now visualise the learned decision boundary in the extended feature space together with the transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJVS_YHD9UyV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the parameters from the optimised model that specify the decision boundary\n",
    "w = clf_3d.coef_[0]\n",
    "\n",
    "b=clf_3d.intercept_[0]\n",
    "\n",
    "fig1=plt.figure(figsize=(10, 10))\n",
    "ax = fig1.add_subplot(projection='3d')\n",
    "ax.scatter3D(X_non_linear[:, 0], X_non_linear[:, 1], r,\n",
    "             c=y_non_linear, s=50, cmap=cmap)\n",
    "ax.view_init(elev=15, azim=60)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('r')\n",
    "\n",
    "# Surface plot of the decision boundary (a plane) from the\n",
    "# optimised parameters\n",
    "xx, yy = np.meshgrid(np.linspace(-1,1,2),np.linspace(-1,1,2))\n",
    "# 0=b+w_0*x+w_1*y+w_2*z  ->  rearrange for z\n",
    "z = -(b + w[0]*xx * w[1]*yy) / w[2]\n",
    "ax.plot_surface(xx, yy, z, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COmUozBL9UyW"
   },
   "source": [
    "\n",
    "Report for **task 2A** the accuracy achieved with the linear SVM in the extended feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V54dBD161KsK"
   },
   "source": [
    "### 2B The Kernel Trick\n",
    "**Note** that in this case it was relatively straight forward to find a suitable new dimension `r` which allows to separate the data linearly. However, this is not feasible for real world datasets. We will now use the RBF kernel on the unchanged data to let the SVM find a proper projection by itself.\n",
    "To use the kernel trick, we need to set a non-linear `kernel`.\n",
    "\n",
    "Use `kernel='rbf'` and fit an `SVC` model on `X_non_linear` and `y_non_linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VfmCiV71KsK"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# create an SVC an set its kernel parameter to 'rbf'\n",
    "# train / fit it on X_non_linear and y_non_linear\n",
    "clf = ...\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_svc_decision_function(clf, X_non_linear, y_non_linear, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCcdm8I38XKr"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_non_linear, y_pred=clf.predict(X_non_linear), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JkHrkdl97W-"
   },
   "source": [
    "\n",
    "What is the accuracy achieved with the SVM with rbf-kernel?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1ddzTzMjk4cf5hJSUYR-rv6YIpX6H6pPZ",
     "timestamp": 1666595151347
    },
    {
     "file_id": "1a8JYfGm1gMPT5yWhnTgQxMueaUIYsmJp",
     "timestamp": 1665570545773
    },
    {
     "file_id": "1pvn6jufgTJDs05Wbx9AL1OWsz8xcf6PR",
     "timestamp": 1664870982154
    },
    {
     "file_id": "12sZ0VrTiGnJUGujCtivupha9XUc5_R4O",
     "timestamp": 1664796966493
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
